# -*- coding: utf-8 -*-
"""Text_processing_and_info_extraction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18cXSLSqD8Uv36iCSlHs8VRTv7Ldpyn8N

## Taking control of happiness: what actions drive happiness?
#### *A preliminary review of actions/verbs that behind happy moments*

HappyDB is a corpus of 100,000 crowd-sourced happy moments via Amazon's Mechanical Turk. In this notebook, we extract verbs from text that represents actions that made people happy at the time of reflection.

### **Step 0: Load required libraries**

From the packages' descriptions:

+ `pandas` is an open source data analysis and manipulation tool;
+ `numpy` is an open source project that enables numerical computing with Python;
+ `spacy` is a free open-source library for Natural Language Processing in Python;
"""

import spacy
import pandas as pd
import numpy as np
from spacy import displacy
from spacy.matcher import DependencyMatcher
from spacy.matcher import Matcher

"""#### **Step 1: Load data to be processed**

Description of key datasets:

+ `hb_df` is the main table with all responses from HappyDB's database;
+ `demo_df` is the self-reported demographic data of respondents;
"""

url_hb = 'https://raw.githubusercontent.com/rit-public/HappyDB/master/happydb/data/cleaned_hm.csv'
url_demo = 'https://raw.githubusercontent.com/rit-public/HappyDB/master/happydb/data/demographic.csv'

hb_df = pd.read_csv(url_hb)
demo_df = pd.read_csv(url_demo)

hb_df.head() # check data

demo_df.head() # check data

"""From steps 2 to x, we will use only two columns from *hb_df* - 'hmid'and 'cleaned_hm' (stored in a pd.df called 'text_corpus'). This is done to minimize computational power needed to analyze the data."""

text_corpus = hb_df[['hmid','cleaned_hm']]

text_corpus.head()

"""#### **Step 2: Process data and check attributes**

**Lemmatization, POS mapping and syntactic dependency mapping**: In this step, we process the text data to extract lemma, POS and parsed dependency mappings for each response. While we don't use this directly our analysis, This step is crucial to understand the underlying structure of data.

We will check structure for a random sample of data from 'text_corpus' (memory constraints).
"""

nlp = spacy.load("en_core_web_sm") # Load spaCy English model and store it in an object 'nlp'

def process_text(text):
    doc = nlp(text)
    token_texts = [token.text for token in doc]
    token_pos = [token.pos_ for token in doc]
    token_lemmas = [token.lemma_ for token in doc]
    token_deps = [token.dep_ for token in doc]

    return token_texts, token_pos, token_lemmas, token_deps

s_size = 50
np.random.seed(8957)
sample_df = text_corpus[['hmid', 'cleaned_hm']].sample(n=s_size, replace=False)
sample_df['index'] = np.arange(1, s_size + 1)

# Apply the function to the 'cleaned_hm' column to extract attributes
sample_df[['token_text', 'token_pos', 'token_lemma', 'token_dep']] = sample_df['cleaned_hm'].apply(process_text).apply(pd.Series)
sample_df.head()

"""Next, we will visualize a few dependencies using displacy visualizor.This is to get an idea of dependencies that exist in a sentence. This part can be skipped if not needed."""

sample_txt = sample_df['cleaned_hm'].iloc[13]

doc = nlp(sample_txt)

displacy.serve(doc, style="dep")

"""#### **Step 3: Extract main actions / phrases from each response**

In this step, we explore two techniques to extract information on actions performed by individuals: Depdendency matching, and information extraction.

**Step 3.1: Identify 'root' verbs or actions using Dependency matching**
"""

# Define patterns - 'root' verb and related direct object OR attributes

pattern = [
    {
        "RIGHT_ID": "root_verb",
        "RIGHT_ATTRS": {"dep": "ROOT", "pos": "VERB"}  # Match ROOT verb
    },
    {
        "LEFT_ID": "root_verb",
        "REL_OP": ">",
        "RIGHT_ID": "object",
        "RIGHT_ATTRS": {"dep": {"in": ["dobj", "attr"]}}  # Match dependent tokens
    }
]

# 'Activate' matcher and assig unique name to pattern
matcher = DependencyMatcher(nlp.vocab)
matcher.add("root_object_pattern_test", [pattern])

# Create an empty list to store results
results = []

for idx in text_corpus.index:
    text = text_corpus.at[idx, 'cleaned_hm']
    doc = nlp(text)
    matches = matcher(doc)

    # Map identifier 'hmid' to each row
    hmid = text_corpus.at[idx, 'hmid']

    for match_id, token_ids in matches:
        chain = " - ".join([doc[token_id].text for token_id in token_ids])
        results.append({"hmid": hmid, "Index": idx, "Root_Object_Phrase": chain})

# Create a DataFrame from the results list
results_df = pd.DataFrame(results)

# Print the DataFrame
results_df.head(20)

"""The pattern revealed some interesting verbs. However, currently, the pattern extracts verbs from responses without categorizing them as action initiated or conrolled by person (experiencer vs agent)

To achieve this, we will simply use 'I' identifier in a response as mapped to the root word.

*Note: This method is a preliminary approach to focus on the focus of the the event or verb. There are many incomplete responses that use incomplete phrases to show action done (such as "Gave an interview today.").*
*Further analysis is needed to find more robust approaches.*

**Step 3.2: Extract 'I' pronoun for each row, if present, and store seperately**
"""

# Create two seperate lists for each column in 'text_corpus'
text_column = text_corpus['cleaned_hm']
hmid_column = text_corpus['hmid']

# Create an empty list to store results
results_ie = []

#Process data
for idx, (text, hmid) in enumerate(zip(text_column, hmid_column)):
    doc = nlp(text)
    pronoun = None

    for token in doc:
        if token.text == 'I' and token.pos_ == 'PRON':
            pronoun = token.text

    # Create a result dictionary
    result = {
        "hmid": hmid,
        "Index": idx,
        "Pronoun": pronoun if pronoun else None
    }

    results_ie.append(result)

result_ie_df = pd.DataFrame(results_ie)
result_ie_df.head(10)

"""#### **Step 4: Merge all datasets**"""

merged_df = results_df.merge(result_ie_df, on = 'hmid', how = 'inner')

merged_df.head() # check data

merged_df = merged_df.merge(hb_df, on = 'hmid', how = 'left')

merged_df = merged_df.merge(demo_df, on = 'wid', how = 'left')

merged_df.head() #check data

print(merged_df.columns.tolist())

# Keep only relevant columns
final_df = merged_df[
    ['hmid', 'Root_Object_Phrase', 'Pronoun', 'wid', 'reflection_period','predicted_category',
     'age', 'country', 'gender', 'marital', 'parenthood']
]